{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b927c729",
   "metadata": {},
   "source": [
    "This notebook loads the benchmarks from HF, with the parallel text from the same benchmark, and saves them to unified dataframes of parallel texts, 1 file per benchmark with multiple columns corresponding to the different languages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0a5c87c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "import pandas as pd\n",
    "import tqdm\n",
    "from datasets import load_dataset\n",
    "from botocore.config import Config\n",
    "from claude import invoke_claude\n",
    "import stanza\n",
    "from tqdm.notebook import tqdm\n",
    "import random\n",
    "import os\n",
    "import ast\n",
    "tqdm.pandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "196cb2ba",
   "metadata": {},
   "source": [
    "# Belebele"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "158fac2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "subsets = ['eng_Latn', 'fra_Latn', 'deu_Latn', 'arb_Arab', 'zho_Hans']\n",
    "datasets = [(load_dataset(\"facebook/belebele\", subset), subset) for subset in subsets if subset != 'All']\n",
    "\n",
    "combined_df = pd.DataFrame()\n",
    "for dataset, subset in datasets:\n",
    "    if subset == \"eng_Latn\":\n",
    "        df = pd.DataFrame(dataset['test']).sort_values(by='link').reset_index(drop=True)[['flores_passage', 'question', 'mc_answer1', 'mc_answer2', 'mc_answer3', 'mc_answer4', 'correct_answer_num']]\n",
    "        df[f\"{subset.split('_')[0]}_flores_passage\"] = df['flores_passage']\n",
    "        df[f\"{subset.split('_')[0]}_question\"] = df['question']\n",
    "        df.drop(columns=['flores_passage', 'question'], inplace=True)\n",
    "        combined_df = pd.concat([combined_df, df], axis=1)\n",
    "    else:\n",
    "        df = pd.DataFrame(dataset['test']).sort_values(by='link').reset_index(drop=True)[['flores_passage', 'question']]\n",
    "        df[f\"{subset.split('_')[0]}_flores_passage\"] = df['flores_passage']\n",
    "        df[f\"{subset.split('_')[0]}_question\"] = df['question']\n",
    "        df.drop(columns=['flores_passage', 'question'], inplace=True)\n",
    "        combined_df = pd.concat([combined_df, df], axis=1)\n",
    "combined_df.reset_index(drop=True, inplace=True)\n",
    "os.makedirs('datasets', exist_ok=True)\n",
    "combined_df.to_csv('./datasets/belebele.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d9f93c6",
   "metadata": {},
   "source": [
    "# MMLU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4cace6b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MMLU dataset saved to 'datasets/mmlu.csv'.\n"
     ]
    }
   ],
   "source": [
    "# Load English questions from the MMLU dataset\n",
    "eng_mmlu = load_dataset(\"cais/mmlu\", \"all\")['test'].to_pandas()\n",
    "# Load the non-English versions (questions) from other provided splits.\n",
    "fra_mmlu = load_dataset(\"openai/MMMLU\", \"FR_FR\")['test'].to_pandas()\n",
    "deu_mmlu = load_dataset(\"openai/MMMLU\", \"DE_DE\")['test'].to_pandas()\n",
    "arb_mmlu = load_dataset(\"openai/MMMLU\", \"AR_XY\")['test'].to_pandas()\n",
    "zho_mmlu = load_dataset(\"openai/MMMLU\", \"ZH_CN\")['test'].to_pandas()\n",
    "\n",
    "combined_mmlu = pd.DataFrame()\n",
    "combined_mmlu['eng_mmlu_question'] = eng_mmlu['question']\n",
    "combined_mmlu['fra_mmlu_question'] = fra_mmlu['Question']  # Note: column name might be different (capitalized).\n",
    "combined_mmlu['deu_mmlu_question'] = deu_mmlu['Question']\n",
    "combined_mmlu['arb_mmlu_question'] = arb_mmlu['Question']\n",
    "combined_mmlu['zho_mmlu_question'] = zho_mmlu['Question']\n",
    "\n",
    "combined_mmlu.reset_index(drop=True, inplace=True)\n",
    "combined_mmlu.to_csv('datasets/mmlu.csv', index=False)\n",
    "print(\"MMLU dataset saved to 'datasets/mmlu.csv'.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43eae64c",
   "metadata": {},
   "source": [
    "# XNLI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "fdb3d159",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XNLI dataset saved to 'datasets/xnli.csv'.\n"
     ]
    }
   ],
   "source": [
    "xnli = load_dataset(\"facebook/xnli\", \"all_languages\")['test'].to_pandas()\n",
    "\n",
    "# For XNLI, the premise and hypothesis columns may be stored as strings representing dictionaries.\n",
    "# Convert them to dictionaries (if needed).\n",
    "xnli['premise'] = xnli['premise'].apply(lambda x: ast.literal_eval(x) if isinstance(x, str) else x)\n",
    "xnli['hypothesis'] = xnli['hypothesis'].apply(lambda x: ast.literal_eval(x) if isinstance(x, str) else x)\n",
    "\n",
    "# Sometimes the hypothesis field contains dictionaries with separate 'language' and 'translation' keys.\n",
    "# Convert such dictionaries into a more usable format.\n",
    "def process_translation(d):\n",
    "    if isinstance(d, dict):\n",
    "        if 'language' in d and 'translation' in d:\n",
    "            return dict(zip(d['language'], d['translation']))\n",
    "    return d\n",
    "\n",
    "xnli['hypothesis'] = xnli['hypothesis'].apply(process_translation)\n",
    "\n",
    "# Define a language map to extract specific language translations.\n",
    "language_map = {\n",
    "    'en': 'eng',\n",
    "    'fr': 'fra',\n",
    "    'de': 'deu',\n",
    "    'ar': 'arb',\n",
    "    'zh': 'zho'\n",
    "}\n",
    "\n",
    "# Extract both the premise and hypothesis for each language.\n",
    "for lang, prefix in language_map.items():\n",
    "    xnli[f'{prefix}_premise'] = xnli['premise'].apply(lambda d: d.get(lang, None) if isinstance(d, dict) else None)\n",
    "    xnli[f'{prefix}_hypothesis'] = xnli['hypothesis'].apply(lambda d: d.get(lang, None) if isinstance(d, dict) else None)\n",
    "\n",
    "xnli.reset_index(drop=True, inplace=True)\n",
    "xnli.to_csv('datasets/xnli.csv', index=False)\n",
    "print(\"XNLI dataset saved to 'datasets/xnli.csv'.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
